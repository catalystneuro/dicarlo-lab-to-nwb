{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of how to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from spikeinterface.extractors import IntanRecordingExtractor\n",
    "from dicarlo_lab_to_nwb.conversion.data_locator import locate_intan_file_path\n",
    "\n",
    "data_folder = Path(\"/media/heberto/One Touch/DiCarlo-CN-data-share\")\n",
    "image_set_name = \"domain-transfer-2023\"\n",
    "subject = \"pico\"\n",
    "session_date = \"20230214\"\n",
    "session_time = \"140610\"\n",
    "\n",
    "intan_file_path = locate_intan_file_path(\n",
    "    data_folder=data_folder,\n",
    "    image_set_name=image_set_name,\n",
    "    subject=subject,\n",
    "    session_date=session_date,\n",
    "    session_time=session_time,\n",
    ")\n",
    "\n",
    "recording = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=\"RHD2000 amplifier channel\",\n",
    "    all_annotations=True,\n",
    "    ignore_integrity_checks=False,\n",
    ")\n",
    "recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular example has timestamps discontinuities, to load the data regardless we set the parameter `ignore_integrity_checks=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=\"RHD2000 amplifier channel\",\n",
    "    all_annotations=True,\n",
    "    ignore_integrity_checks=True,\n",
    ")\n",
    "recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_auxiliary_input = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=\"RHD2000 auxiliary input channel\",\n",
    "    all_annotations=True,\n",
    "    ignore_integrity_checks=True,\n",
    ")\n",
    "\n",
    "recording_auxiliary_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADC input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_adc_input = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=\"USB board ADC input channel\",\n",
    "    all_annotations=True,\n",
    "    ignore_integrity_checks=True,\n",
    ")\n",
    "\n",
    "recording_adc_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digital channel \n",
    "Requires neo version from github https://github.com/NeuralEnsemble/python-neo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_digital = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=\"USB board digital input channel\",\n",
    "    all_annotations=True,\n",
    "    ignore_integrity_checks=True,\n",
    ")\n",
    "\n",
    "recording_digital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicarlo_lab_to_nwb.conversion.probe import build_probe_group\n",
    "from dicarlo_lab_to_nwb.conversion.data_locator import locate_intan_file_path\n",
    "from spikeinterface.extractors import IntanRecordingExtractor\n",
    "\n",
    "data_folder = Path(\"/media/heberto/One Touch/DiCarlo-CN-data-share\")\n",
    "image_set_name = \"domain-transfer-2023\"\n",
    "subject = \"pico\"\n",
    "session_date = \"20230214\"\n",
    "session_time = \"140610\"\n",
    "\n",
    "\n",
    "intan_file_path = locate_intan_file_path(\n",
    "    data_folder=data_folder,\n",
    "    image_set_name=image_set_name,\n",
    "    subject=subject,\n",
    "    session_date=session_date,\n",
    "    session_time=session_time,\n",
    ")\n",
    "\n",
    "\n",
    "stream_name = \"RHD2000 amplifier channel\"\n",
    "recording = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=stream_name,\n",
    "    ignore_integrity_checks=True,\n",
    "    all_annotations=True,\n",
    ")\n",
    "\n",
    "\n",
    "probe_group = build_probe_group(recording=recording)\n",
    "\n",
    "\n",
    "from probeinterface.plotting import plot_probe\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "probe = probe_group.probes[0]\n",
    "channel_ids = recording.get_channel_ids()\n",
    "corresponding_channel_ids = [channel_ids[i] for i in probe.device_channel_indices]\n",
    "\n",
    "text_on_contact = np.asarray(corresponding_channel_ids)\n",
    "\n",
    "plot_probe(probe=probe, ax=ax, with_contact_id=True, text_on_contact=text_on_contact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probeinterface.plotting import plot_probe_group\n",
    "\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "plot_probe_group(probe_group, ax=ax, same_axes=True, with_contact_id=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a sorting pipeline we need a recording with a geometry attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.extractors import IntanRecordingExtractor\n",
    "from spikeinterface.sorters import run_sorter_by_property\n",
    "\n",
    "\n",
    "from dicarlo_lab_to_nwb.conversion.data_locator import locate_intan_file_path\n",
    "from dicarlo_lab_to_nwb.conversion.probe import attach_probe_to_recording\n",
    "\n",
    "data_folder = Path(\"/media/heberto/One Touch/DiCarlo-CN-data-share\")\n",
    "image_set_name = \"domain-transfer-2023\"\n",
    "subject = \"pico\"\n",
    "session_date = \"20230214\"\n",
    "session_time = \"140610\"\n",
    "\n",
    "\n",
    "intan_file_path = locate_intan_file_path(\n",
    "    data_folder=data_folder,\n",
    "    image_set_name=image_set_name,\n",
    "    subject=subject,\n",
    "    session_date=session_date,\n",
    "    session_time=session_time,\n",
    ")\n",
    "\n",
    "\n",
    "stream_name = \"RHD2000 amplifier channel\"\n",
    "recording = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=stream_name,\n",
    "    ignore_integrity_checks=True,\n",
    "    all_annotations=True,\n",
    ")\n",
    "\n",
    "\n",
    "attach_probe_to_recording(recording=recording)\n",
    "recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sorters have been designed with high density probes in mind. They will work with a single channel probe, but the results may not be as good as some units might be supressed by the spatial regularization.\n",
    "\n",
    "Because of this we performed sorting in two ways so you can compare the results:\n",
    "\n",
    "1. We do one sorting per probe\n",
    "2. We do one sorting per channel to avoid interference of the spatial regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a sorting per probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import load_extractor\n",
    "\n",
    "sorting_folder = Path(\"./sorting_done\")\n",
    "overwrite = False\n",
    "\n",
    "if sorting_folder.exists() and not overwrite:\n",
    "    sorting = load_extractor(sorting_folder)\n",
    "else:\n",
    "    sorting = run_sorter_by_property(\n",
    "        sorter_name=\"kilosort2\",\n",
    "        recording=recording,\n",
    "        folder=\"./sorting_folder_probe\",\n",
    "        grouping_property=\"probe\",\n",
    "        docker_image=True,\n",
    "    )\n",
    "\n",
    "    sorting.save(folder=sorting_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action item:\n",
    "* How to save the sorting results to numpy\n",
    "* Quality metrics:\n",
    "    * Which channels are visually driven? we repeat the image 20 times, we randomly choose the first 10 images of the set same of itme\n",
    "and correlate the responses.\n",
    "    *   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import create_sorting_analyzer\n",
    "\n",
    "\n",
    "sorting_analyzer = create_sorting_analyzer(sorting=sorting, recording=recording)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing a sorting per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = run_sorter_by_property(\n",
    "    sorter_name=\"kilosort3\",\n",
    "    recording=recording,\n",
    "    folder=\"./sorting_folder_per_channel\",\n",
    "    grouping_property=\"channel_names\",\n",
    "    docker_image=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.sorters import available_sorters\n",
    "\n",
    "available_sorters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.core import load_extractor\n",
    "\n",
    "sorting_folder = Path(\"./sorting_done_per_channel\")\n",
    "overwrite = False\n",
    "\n",
    "if sorting_folder.exists() and not overwrite:\n",
    "    sorting = load_extractor(sorting_folder)\n",
    "else:\n",
    "    sorting = run_sorter_by_property(\n",
    "        sorter_name=\"tridesclous\",\n",
    "        recording=recording,\n",
    "        folder=\"./sorting_folder_per_channel\",\n",
    "        grouping_property=\"channel_names\",\n",
    "        docker_image=True,\n",
    "    )\n",
    "\n",
    "    sorting.save(folder=sorting_folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.widgets as sw\n",
    "\n",
    "from spikeinterface.core.generate import generate_ground_truth_recording\n",
    "\n",
    "\n",
    "recording, sorting = generate_ground_truth_recording(num_channels=4, num_units=1, durations=[1], seed=0)\n",
    "\n",
    "\n",
    "w_ts = sw.plot_traces(recording, time_range=(0, 1))\n",
    "w_rs = sw.plot_rasters(sorting, time_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dicarlo_lab_to_nwb.conversion.pipeline import di_carlo_peak_detection\n",
    "\n",
    "\n",
    "\n",
    "job_kwargs = dict(n_jobs=1, verbose=True, progress_bar=True, chunk_duration=1.0)\n",
    "noise_threshold = 3  # The number of standard deviations for peak detection\n",
    "\n",
    "spike_times_per_channel = di_carlo_peak_detection(recording=recording, noise_threshold=noise_threshold, job_kwargs=job_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting.get_unit_spike_train(0, return_times=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times_per_channel[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import spikeinterface.widgets as sw\n",
    "from dicarlo_lab_to_nwb.conversion.data_locator import locate_intan_file_path\n",
    "\n",
    "from spikeinterface.extractors import IntanRecordingExtractor\n",
    "\n",
    "\n",
    "data_folder = Path(\"/media/heberto/One Touch/DiCarlo-CN-data-share\")\n",
    "image_set_name = \"domain-transfer-2023\"\n",
    "subject = \"pico\"\n",
    "session_date = \"20230214\"\n",
    "session_time = \"140610\"\n",
    "\n",
    "\n",
    "intan_file_path = locate_intan_file_path(\n",
    "    data_folder=data_folder,\n",
    "    image_set_name=image_set_name,\n",
    "    subject=subject,\n",
    "    session_date=session_date,\n",
    "    session_time=session_time,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "recording = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=\"RHD2000 amplifier channel\",\n",
    "    all_annotations=True,\n",
    "    ignore_integrity_checks=True,\n",
    ")\n",
    "\n",
    "# If you want to select only one channel\n",
    "channel_ids = recording.get_channel_ids()[0:1]\n",
    "recording = recording.select_channels(channel_ids=channel_ids)\n",
    "w_ts = sw.plot_traces(recording, time_range=(0, 1), return_scaled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicarlo_lab_to_nwb.conversion.pipeline import DiCarloBandPass, DiCarloNotch\n",
    "\n",
    "\n",
    "f_notch = 60  # Hz\n",
    "bandwidth = 10\n",
    "f_low = 300.0\n",
    "f_high = 6000.0\n",
    "\n",
    "vectorized = True \n",
    "notched_recording = DiCarloNotch(recording, f_notch=f_notch, bandwidth=bandwidth, vectorized=vectorized)\n",
    "preprocessed_recording = DiCarloBandPass(notched_recording, f_low=f_low, f_high=f_high, vectorized=vectorized)\n",
    "\n",
    "# For this instance each array 96 channels, 400 micrometes apart\n",
    "w_ts = sw.plot_traces(preprocessed_recording, time_range=(0, 1), return_scaled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the peak detection on a short portion of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dicarlo_lab_to_nwb.conversion.pipeline import di_carlo_peak_detection\n",
    "\n",
    "noise_threshold = 3  # The number of standard deviations for peak detection\n",
    "\n",
    "start_frame = 0\n",
    "seconds_of_data = 1.0\n",
    "end_frame = int(preprocessed_recording.sampling_frequency * seconds_of_data)\n",
    "preprocessed_recording = preprocessed_recording.frame_slice(start_frame=start_frame, end_frame=end_frame)\n",
    "\n",
    "spike_times_per_channel = di_carlo_peak_detection(\n",
    "    recording=preprocessed_recording,\n",
    "    noise_threshold=noise_threshold,\n",
    ")\n",
    "\n",
    "spike_times_per_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Everything can be wrapped up in a couple of lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.extractors import IntanRecordingExtractor\n",
    "from dicarlo_lab_to_nwb.conversion.pipeline import thresholding_pipeline\n",
    "from dicarlo_lab_to_nwb.conversion.data_locator import locate_intan_file_path\n",
    "\n",
    "\n",
    "image_set_name = \"domain-transfer-2023\"\n",
    "subject = \"pico\"\n",
    "session_date = \"20230214\"\n",
    "session_time = \"140610\"\n",
    "\n",
    "# Parameters of the pipeline\n",
    "f_notch = 60  # Hz\n",
    "bandwidth = 10\n",
    "f_low = 300.0\n",
    "f_high = 6000.0\n",
    "noise_threshold = 3  # The number of standard deviations for peak detection\n",
    "\n",
    "\n",
    "data_folder = Path(\"/media/heberto/One Touch/DiCarlo-CN-data-share\")\n",
    "\n",
    "intan_file_path = locate_intan_file_path(\n",
    "    data_folder=data_folder,\n",
    "    image_set_name=image_set_name,\n",
    "    subject=subject,\n",
    "    session_date=session_date,\n",
    "    session_time=session_time,\n",
    ")\n",
    "\n",
    "\n",
    "stream_name = \"RHD2000 amplifier channel\"\n",
    "recording = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=stream_name,\n",
    "    ignore_integrity_checks=True,\n",
    "    all_annotations=True,\n",
    ")\n",
    "\n",
    "spike_times_per_channel_vectorized = thresholding_pipeline(\n",
    "    recording=recording.frame_slice(start_frame=0, end_frame=1000), # Remove frame_slice to run the whole pipeline\n",
    "    f_notch=f_notch,\n",
    "    bandwidth=bandwidth,\n",
    "    f_low=f_low,\n",
    "    f_high=f_high,\n",
    "    noise_threshold=noise_threshold,\n",
    ")\n",
    "\n",
    "spike_times_per_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from spikeinterface.extractors import IntanRecordingExtractor\n",
    "from dicarlo_lab_to_nwb.conversion.pipeline import thresholding_pipeline\n",
    "\n",
    "image_set_name = \"domain-transfer-2023\"\n",
    "subject = \"pico\"\n",
    "session_date = \"20230214\"\n",
    "session_time = \"140610\"\n",
    "\n",
    "# Parameters of the pipeline\n",
    "f_notch = 60  # Hz\n",
    "bandwidth = 10\n",
    "f_low = 300.0\n",
    "f_high = 6000.0\n",
    "noise_threshold = 3  # The number of standard deviations for peak detection\n",
    "\n",
    "data_folder = Path(\"/media/heberto/One Touch/DiCarlo-CN-data-share\")\n",
    "intan_file_path = locate_intan_file_path(\n",
    "    data_folder=data_folder,\n",
    "    image_set_name=image_set_name,\n",
    "    subject=subject,\n",
    "    session_date=session_date,\n",
    "    session_time=session_time,\n",
    ")\n",
    "\n",
    "\n",
    "stream_name = \"RHD2000 amplifier channel\"\n",
    "recording = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=stream_name,\n",
    "    ignore_integrity_checks=True,\n",
    "    all_annotations=True,\n",
    ")\n",
    "\n",
    "\n",
    "recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline vectorized vs non-vectorized\n",
    "We do the comparision with some seconds of data only, here we only use chunks of 1 seconds and we parallelize over 1 seconds chunks\n",
    "to showcase the speedup of the vectorized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_frame = 0\n",
    "seconds_of_data = 16.0\n",
    "end_frame = int(recording.sampling_frequency * seconds_of_data)\n",
    "recording_short = recording.frame_slice(start_frame=start_frame, end_frame=end_frame)\n",
    "recording_to_use = recording_short\n",
    "\n",
    "vectorized = True\n",
    "job_kwargs = dict(n_jobs=1, progress_bar=True, verbose=True, chunk_duration=1.0)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "spike_times_per_channel_vectorized = thresholding_pipeline(\n",
    "    recording=recording_to_use,\n",
    "    f_notch=f_notch,\n",
    "    bandwidth=bandwidth,\n",
    "    f_low=f_low,\n",
    "    f_high=f_high,\n",
    "    noise_threshold=noise_threshold,\n",
    "    vectorized=vectorized,\n",
    "    job_kwargs=job_kwargs,\n",
    ")\n",
    "\n",
    "time_stop = time.time()\n",
    "time_taken = time_stop - time_start\n",
    "print(f\"Time elapsed: {time_taken} seconds\")\n",
    "\n",
    "time_vectorized = time_taken\n",
    "\n",
    "\n",
    "vectorized = False\n",
    "time_start = time.time()\n",
    "\n",
    "spike_times_per_channel = thresholding_pipeline(\n",
    "    recording=recording_to_use,\n",
    "    f_notch=f_notch,\n",
    "    bandwidth=bandwidth,\n",
    "    f_low=f_low,\n",
    "    f_high=f_high,\n",
    "    noise_threshold=noise_threshold,\n",
    "    vectorized=vectorized,\n",
    "    job_kwargs=job_kwargs,\n",
    ")\n",
    "\n",
    "time_stop = time.time()\n",
    "time_taken = time_stop - time_start\n",
    "print(f\"Time elapsed: {time_taken} seconds\")\n",
    "\n",
    "time_non_vectorized = time_taken\n",
    "\n",
    "speedup = time_non_vectorized/time_vectorized\n",
    "print(f\"Speedup: {speedup:.2f} (times faster)\" )\n",
    "\n",
    "# Test that the results are the same\n",
    "for channel_index, spike_times in spike_times_per_channel.items():\n",
    "    spike_times_vectorized = spike_times_per_channel_vectorized[channel_index]\n",
    "    assert np.allclose(spike_times, spike_times_vectorized), f\"Channel {channel_index} spike times do not match\"\n",
    "    \n",
    "recording_to_use = recording_short\n",
    "vectorized = True\n",
    "\n",
    "job_kwargs = dict(n_jobs=-1, progress_bar=True, verbose=True, chunk_duration=1.0)\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "spike_times_per_channel = thresholding_pipeline(\n",
    "    recording=recording_to_use,\n",
    "    f_notch=f_notch,\n",
    "    bandwidth=bandwidth,\n",
    "    f_low=f_low,\n",
    "    f_high=f_high,\n",
    "    noise_threshold=noise_threshold,\n",
    "    vectorized=vectorized,\n",
    "    job_kwargs=job_kwargs,\n",
    ")\n",
    "\n",
    "time_stop = time.time()\n",
    "time_taken = time_stop - time_start\n",
    "print(f\"Time elapsed: {time_taken} seconds\")\n",
    "\n",
    "time_vectorized_multiprocessing  = time_taken\n",
    "\n",
    "speedup = time_non_vectorized/time_vectorized_multiprocessing\n",
    "print(f\"Speedup: {speedup:.2f} (times faster)\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full recording parallelized in multiple cores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_to_use = recording\n",
    "vectorized = True\n",
    "\n",
    "# Note that is using as many cores as chunks (n_jobs=num_chunks)\n",
    "chunk_duration = 1.0\n",
    "job_kwargs = dict(n_jobs=-1, verbose=True, progress_bar=True, chunk_size=chunk_duration)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "spike_times_per_channel = thresholding_pipeline(\n",
    "    recording=recording_to_use,\n",
    "    f_notch=f_notch,\n",
    "    bandwidth=bandwidth,\n",
    "    f_low=f_low,\n",
    "    f_high=f_high,\n",
    "    noise_threshold=noise_threshold,\n",
    "    vectorized=vectorized,\n",
    "    job_kwargs=job_kwargs,\n",
    ")\n",
    "\n",
    "time_stop = time.time()\n",
    "time_taken = time_stop - time_start\n",
    "print(f\"Time elapsed: {time_taken} seconds\")\n",
    "\n",
    "time_multiprocessing = time_taken\n",
    "\n",
    "\n",
    "job_kwargs = dict(n_jobs=1, verbose=True, progress_bar=True, chunk_size=chunk_duration)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "spike_times_per_channel = thresholding_pipeline(\n",
    "    recording=recording_to_use,\n",
    "    f_notch=f_notch,\n",
    "    bandwidth=bandwidth,\n",
    "    f_low=f_low,\n",
    "    f_high=f_high,\n",
    "    noise_threshold=noise_threshold,\n",
    "    vectorized=vectorized,\n",
    "    job_kwargs=job_kwargs,\n",
    ")\n",
    "\n",
    "time_stop = time.time()\n",
    "time_taken = time_stop - time_start\n",
    "print(f\"Time elapsed: {time_taken} seconds\")\n",
    "\n",
    "time_single_core = time_taken\n",
    "\n",
    "speedup = time_single_core/time_multiprocessing\n",
    "\n",
    "print(f\"Speedup: {speedup:.2f} (times faster)\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = (time_non_vectorized * recording_to_use.get_duration() * 0.10 )/time_multiprocessing\n",
    "\n",
    "print(f\"Speedup with respect to non-vectorized version: {speedup:.2f} (times faster)\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full pipeline spikeinterface vs DiCarlo form Intan and NWB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load NWB Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math \n",
    "\n",
    "\n",
    "folder_path = Path.home() / \"conversion_nwb\" \n",
    "# folder_path = folder_path / \"nwb_stub\"\n",
    "assert folder_path.is_dir()\n",
    "\n",
    "file_path = folder_path / \"pico_20230214_140610.nwb\"\n",
    "assert file_path.is_file()\n",
    "\n",
    "from spikeinterface.extractors import NwbRecordingExtractor\n",
    "\n",
    "recording = NwbRecordingExtractor(file_path=file_path)\n",
    "\n",
    "\n",
    "recording_to_use = recording\n",
    "vectorized = True\n",
    "\n",
    "\n",
    "job_kwargs = dict(n_jobs=-1, verbose=True, progress_bar=True, chunk_duration=1.0)\n",
    "\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "spike_times_per_channel = thresholding_pipeline(\n",
    "    recording=recording_to_use,\n",
    "    f_notch=f_notch,\n",
    "    bandwidth=bandwidth,\n",
    "    f_low=f_low,\n",
    "    f_high=f_high,\n",
    "    noise_threshold=noise_threshold,\n",
    "    vectorized=vectorized,\n",
    "    job_kwargs=job_kwargs,\n",
    ")\n",
    "\n",
    "time_stop = time.time()\n",
    "time_taken = time_stop - time_start\n",
    "print(f\"Time elapsed: {time_taken} seconds\")\n",
    "\n",
    "time_nwb = time_taken\n",
    "\n",
    "speedup = time_multiprocessing/time_nwb\n",
    "print(f\"Speedup: {speedup:.2f} (times faster)\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a faithful comparison by using 10 chunks\n",
    "This can't use multiprocessing because is uses too much memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from spikeinterface.extractors import IntanRecordingExtractor\n",
    "from dicarlo_lab_to_nwb.conversion.pipeline import thresholding_pipeline\n",
    "\n",
    "image_set_name = \"domain-transfer-2023\"\n",
    "subject = \"pico\"\n",
    "session_date = \"20230214\"\n",
    "session_time = \"140610\"\n",
    "\n",
    "# Parameters of the pipeline\n",
    "f_notch = 60  # Hz\n",
    "bandwidth = 10\n",
    "f_low = 300.0\n",
    "f_high = 6000.0\n",
    "noise_threshold = 3  # The number of standard deviations for peak detection\n",
    "\n",
    "data_folder = Path(\"/media/heberto/One Touch/DiCarlo-CN-data-share\")\n",
    "image_set_name = \"domain-transfer-2023\"\n",
    "subject = \"pico\"\n",
    "session_date = \"20230214\"\n",
    "session_time = \"140610\"\n",
    "\n",
    "\n",
    "intan_file_path = locate_intan_file_path(\n",
    "    data_folder=data_folder,\n",
    "    image_set_name=image_set_name,\n",
    "    subject=subject,\n",
    "    session_date=session_date,\n",
    "    session_time=session_time,\n",
    ")\n",
    "\n",
    "stream_name = \"RHD2000 amplifier channel\"\n",
    "recording = IntanRecordingExtractor(\n",
    "    file_path=intan_file_path,\n",
    "    stream_name=stream_name,\n",
    "    ignore_integrity_checks=True,\n",
    "    all_annotations=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "recording_to_use = recording\n",
    "vectorized = True\n",
    "\n",
    "# Note that is using as many cores as chunks (n_jobs=num_chunks)\n",
    "num_chunks = 10\n",
    "chunk_size =  math.ceil(recording.get_num_samples() / num_chunks)\n",
    "\n",
    "###  If you have a lot of memory you can test adding more jobs, example code on how to get the number of cores.\n",
    "# import psutil  # You have the pip install psutil\n",
    "# num_cores = psutil.cpu_count(logical=True)\n",
    "# n_jobs = min(num_cores, num_chunks)\n",
    "\n",
    "job_kwargs = dict(n_jobs=1, verbose=True, progress_bar=True, chunk_size=chunk_duration)\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "spike_times_per_channel = thresholding_pipeline(\n",
    "    recording=recording_to_use,\n",
    "    f_notch=f_notch,\n",
    "    bandwidth=bandwidth,\n",
    "    f_low=f_low,\n",
    "    f_high=f_high,\n",
    "    noise_threshold=noise_threshold,\n",
    "    vectorized=vectorized,\n",
    "    job_kwargs=job_kwargs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroconv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
